This study investigates the process of simulating what is heard by one when they speak (which is heavily influenced by the process of bone conduction) through pre-recorded voice signals, captured via air conduction.   

Using python, Fast Fourier Transform and Inverse Fast Fourier Transform was used to convert the sound signals from the time domain into the frequency domain and back.   

The code uses Butterworth filters within the frequency domain to modulate the amplitude of signals within a given range of frequency. Specifically, they are modulated on 4 distinct bands: [100, 800] Hz, [800, 2000] Hz, [2000, 4000] Hz, and [4000, 10000] Hz. These filters serve as the core of the simulation, as they model and reflect the known physiological differences in how sound propagates through bone, particularly emphasizing the lower and middle-frequency amplification. The findings, presented through time-domain and frequency-domain plots and synthesized audio, demonstrates an easy yet practical way of transforming externally recorded signals into approximations of those perceived internally by oneself.
